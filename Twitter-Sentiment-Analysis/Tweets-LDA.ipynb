{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In natural language processing, the Latent Dirichlet Allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.\n",
    "We would like to determine the number of topics on tweets from Egypt collected in 1st Decemeber 2022."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory data analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create the spark app "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, udf, lit, col, size, expr\n",
    "\n",
    "spark = SparkSession.builder.appName('read JSON files').getOrCreate()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Read all the fetched tweets Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecTweets/WCTweets365.json\n"
     ]
    }
   ],
   "source": [
    "json_df = spark.read.option(\"multiline\", \"true\").json(\n",
    "    \"DecTweets/WCTweets.json\")\n",
    "\n",
    "for i in range(2, 724):\n",
    "    path = \"DecTweets/WCTweets\"+str(i)+\".json\"\n",
    "    print(path)\n",
    "    temp_df = spark.read.option(\"multiline\", \"true\").json(path=path)\n",
    "    json_df = json_df.unionByName(temp_df, allowMissingColumns=True)\n",
    "    json_df.count()\n",
    "    temp_df=None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Show the number of files in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 View the dataframe schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 View the ***data*** attribute schema from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.select('data').printSchema()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create a new dataframe ***data_df*** that contains only the data attribute of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data column from the json_df, and adds a new column to the data_df called \"data\"\n",
    "# that column has an alias called \"data\", and each row has the data of only one objects.\n",
    "data_df=json_df.select('data').withColumn('data', explode('data').alias('data'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 View the new dataframe schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.printSchema()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Show the number of tweets in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.count()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Selecting only the inner attributes of the ***data*** attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.select('data.author_id',\n",
    "                         'data.created_at',\n",
    "                         'data.geo.place_id',\n",
    "                         'data.id',\n",
    "                         'data.public_metrics',\n",
    "                         'data.text')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4 Viewing the updated schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the schema after selection\n",
    "data_df.printSchema()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Create a new dataframe ***place_df*** that contains only the *includes.places* attribute of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the place content\n",
    "place_df=json_df.select('includes.places')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1 View the place_df Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the schema fo place\n",
    "place_df.schema.names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Create a new dataframe ***user_df*** that contains only the *includes.users* attribute of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the user content\n",
    "user_df=json_df.select('includes.users')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6.1 View the *users_df* schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.printSchema()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Unravel the nested json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import col\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.1 Define a function to read the nested json structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nested_json(df):\n",
    "    column_list = []\n",
    "    # Iterate over all the column names in the schema\n",
    "    for column_name in df.schema.names:\n",
    "        # Check if the column is of \"ArrayType\"\n",
    "        if isinstance(df.schema[column_name].dataType, T.ArrayType):\n",
    "            # Replace the column of \"ArrayType\" with an exploded version of that column\n",
    "            df = df.withColumn(column_name, explode(column_name).alias(column_name))\n",
    "            # Append the column name to an array of column list array\n",
    "            column_list.append(column_name)\n",
    "\n",
    "        # Check if the column is of \"StructType\"\n",
    "        elif isinstance(df.schema[column_name].dataType, T.StructType):\n",
    "            # Iterate over all fields of this struct\n",
    "            for field in df.schema[column_name].dataType.fields:\n",
    "                # Append a new column made of the column name and the field name to the column list array \n",
    "                column_list.append(col(column_name + \".\" + field.name).alias(column_name + \"_\" + field.name))\n",
    "        \n",
    "        # If the column is neither an \"ArrayType\" nor a \"StructType\" aka \"Leaf\" attribute\n",
    "        # eg:\"String\" , \"Long\", etc.\n",
    "        # add this column to the column list array\n",
    "        else:\n",
    "            column_list.append(column_name)\n",
    "\n",
    "    # Selecting columns using column_list from dataframe: df\n",
    "    df = df.select(column_list)\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.2 Define a function to flatten the nested JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a recursive function to allow the flattening \n",
    "# of nested \"ArrayTypes\" or \"StructTypes\" \n",
    "def flatten_nested_json(df):\n",
    "    read_nested_json_flag = True\n",
    "    while read_nested_json_flag:\n",
    "        df = read_nested_json(df)\n",
    "        read_nested_json_flag = False\n",
    "        \n",
    "        # Check if there is still an \"ArrayType\" or \"StructType\" after the \n",
    "        # reading itertaion and if there is, iterate over the json \n",
    "        # structure again until there are no \"ArrayType\" or \"StructType\" \n",
    "        # in the dataframe, only primitive types.\n",
    "        for column_name in df.schema.names:\n",
    "            if isinstance(df.schema[column_name].dataType, T.ArrayType):\n",
    "              read_nested_json_flag = True\n",
    "            elif isinstance(df.schema[column_name].dataType, T.StructType):\n",
    "              read_nested_json_flag = True\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.3 Flatten the Data df and show the updated schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=flatten_nested_json(data_df)\n",
    "\n",
    "data_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.4 Perform different sql queries on the data dataframe to learn more information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of record in data_df dataframe\n",
    "data_df.createOrReplaceTempView(\"df_data\")\n",
    "spark.sql(\"SELECT count(*) FROM df_data\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the id is the primary key\n",
    "spark.sql(\"SELECT count(DISTINCT id) FROM df_data\").show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.5 Flatten the Place df and show the updated schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_df=flatten_nested_json(place_df)\n",
    "place_df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.6 Perform different sql queries on the dataframe to learn more information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of unique place in place_df dataframe\n",
    "place_df.createOrReplaceTempView(\"df_place\")\n",
    "spark.sql(\"SELECT count(*) FROM df_place\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the tweet location for first 100 records\n",
    "place_df.show(10, False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.7 Flatten the Users df and show the updated schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the neseted json of user\n",
    "user_df=flatten_nested_json(user_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7.8 Perform different sql queries on the dataframe to learn more information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of unique user in user_df dataframe\n",
    "user_df.createOrReplaceTempView(\"df_user\")\n",
    "spark.sql(\"SELECT count(DISTINCT users_id) FROM df_user\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the tweet user of first 100 records\n",
    "user_df.show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DISTINCT(users_name,users_location) FROM df_user where users_location is not null\").show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from gensim.models import LsiModel\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2 Create a new dataframe that contains only the tweet id, creation date and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = data_df.select('id','created_at','text')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 View the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.show(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 View the new dataframe schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.printSchema()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Preprocessing and Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess steps\n",
    "import re\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "import pyspark.sql.types as T\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Change the *created_at* attribute from **String** to **DateType**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text=tweet_text.withColumn(\"created_date\", tweet_text['created_at'].cast(T.DateType()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Define the UDFs for the cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub('\\[.*?\\]', '', text)  # remove square brackets\n",
    "text = re.sub('[%s]' % re.escape(string.punctuation),\n",
    "              '', text)  # remove puncitutations marks\n",
    "text = re.sub('\\w*\\d\\w*', '', text)  # remove words that contain numbers\n",
    "text = re.sub('[‘’“”…]', '', text)  # remove quotes\n",
    "text = re.sub('\\r', '', text)  # remove \\r\n",
    "text = re.sub('\\n', '', text)  # remove \\n\n",
    "text = re.sub('\\t', '', text)  # remove \\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n",
    "\n",
    "def removeLinks(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) \n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) \n",
    "    tweet = tweet.strip('[link]') \n",
    "    return tweet\n",
    "def removeMentions(tweet):\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    return tweet\n",
    "def removePunctuation(tweet):\n",
    "    tweet = re.sub('['+ PUNCTUATION + ']+', ' ', tweet) \n",
    "    return tweet\n",
    "def removeNumbers(tweet):\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) \n",
    "    return tweet\n",
    "\n",
    "def removeBreakLines(tweet):\n",
    "    tweet=tweet.strip().replace('\\n', '')\n",
    "    tweet=tweet.strip().replace('\\r', '')\n",
    "    tweet = tweet.strip().replace('\\t', '')\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def remove_emoji(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register user defined function\n",
    "removeLinks = udf(removeLinks)\n",
    "removeMentions = udf(removeMentions)\n",
    "removePunctuation=udf(removePunctuation)\n",
    "removeNumbers=udf(removeNumbers)\n",
    "removeBreakLines= udf(removeBreakLines)\n",
    "remove_emoji=udf(remove_emoji)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 Remove Links, User Mentions, Punctuation and Numbers from *tweet_text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text=tweet_text.withColumn('cleaned_Text', removeLinks(tweet_text['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text=tweet_text.withColumn('cleaned_Text', removeMentions(tweet_text['cleaned_Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text=tweet_text.withColumn('cleaned_Text', removePunctuation(tweet_text['cleaned_Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text=tweet_text.withColumn('cleaned_Text', removeNumbers(tweet_text['cleaned_Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text=tweet_text.withColumn('cleaned_Text', removeBreakLines(tweet_text['cleaned_Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text=tweet_text.withColumn('cleaned_Text', remove_emoji(tweet_text['cleaned_Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text.select('text','cleaned_Text').show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.5 Create a tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tokenizer that matches any tokens that contains characters,\n",
    "# with a minimum length of 3 characters, from the cleaned_Text,\n",
    "# and output the tokens in the tokens output.\n",
    "\n",
    "tokenizer = RegexTokenizer().setPattern(\"[\\\\W_]+\").setMinTokenLength(3).setInputCol(\"cleaned_Text\").setOutputCol(\"tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = tokenizer.transform(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original text and the clenaed text \n",
    "# and the tokenized cleaned tweets.\n",
    "tokenized_tweets.select('text','cleaned_Text','tokens').show(50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.6 Create a WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.6.1 Create a lemmatizing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Lemmatizing function that takes each row \n",
    "# and lemmatize each word to a verb and return the \n",
    "# row containing the lemmatized words.\n",
    "\n",
    "def lemmatization(row):\n",
    "    row = [lemmatizer.lemmatize(word,'v') for word in row]\n",
    "    return row\n",
    "\n",
    "\n",
    "lemmatize = udf(lemmatization)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6.6.2 Create a new column that contains the lemmatized tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_tweets=tokenized_tweets.withColumn('tokens_lemma', lemmatize(tokenized_tweets['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets.select('cleaned_Text','tokens','tokens_lemma').show(50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.7 Removing Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_stopwords = open('TwitterStopWords.txt', 'r').read().split(\",\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.7.1 Create a new stopwords list from twitter stop list and the nltk wordnet list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordList=StopWordsRemover().getStopWords()\n",
    "stopwordList.extend(twitter_stopwords)\n",
    "stopwordList = list(set(stopwordList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new stopWordsRemover with the stopwords as our \n",
    "# extended stopwordsList, the input from the tokens column,\n",
    "# and output to cleaned_tokens column.\n",
    "remover = StopWordsRemover(stopWords=stopwordList).setInputCol(\"tokens\").setOutputCol(\"cleaned_tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = remover.transform(tokenized_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector of words that at least appeared in two different tweets, and set maximum vocab size to 20000.\n",
    "vectorizer = CountVectorizer().setInputCol(\"cleaned_tokens\").setOutputCol(\"features\").setVocabSize(20000).setMinDF(2).fit(cleaned_tweets)\n",
    "wordVectors = vectorizer.transform(cleaned_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors.select(\"features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of topic\n",
    "# set the mertic to evaluate model performance\n",
    "num_topics = range(2, 11)\n",
    "models = []\n",
    "log_likeli = []\n",
    "log_perp = []\n",
    "for num in num_topics:\n",
    "    # LDA\n",
    "    # create Latent Dirichlet Allocation model and run it on our data with 50 iteration and selected topics number\n",
    "    lda = LDA(k=num, maxIter=50)\n",
    "    # fit the model on data\n",
    "    ldaModel = lda.fit(wordVectors)\n",
    "    models.append(ldaModel)\n",
    "    ll = ldaModel.logLikelihood(wordVectors)\n",
    "    lp = ldaModel.logPerplexity(wordVectors)\n",
    "    log_likeli.append(ll)\n",
    "    log_perp.append(lp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plot_data=pd.DataFrame(list(zip(num_topics,log_likeli,log_perp)),\n",
    "            columns=['topics_num','logLikelihood','logPerplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the elbow method to determine the optimal k\n",
    "plot_data.plot(x='topics_num', y='logLikelihood', kind='line')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the elbow method to determine the optimal k\n",
    "plot_data.plot(x='topics_num', y='logPerplexity', kind='line')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "# create Latent Dirichlet Allocation model and run it on our data with 50 iteration and selected topics number\n",
    "lda = LDA(k=4, maxIter=100)\n",
    "# fit the model on data\n",
    "model = lda.fit(wordVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = model.logLikelihood(wordVectors)\n",
    "lp = model.logPerplexity(wordVectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The lower bound on the log likelihood of the entire corpus: \",ll)\n",
    "print(\"The upper bound on perplexity: \",lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract vocabulary from CountVectorizer\n",
    "\n",
    "vocab = vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create topics based on LDA\n",
    "lda_topics = model.describeTopics()\n",
    "lda_topics.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics.select('termWeights').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics.select('termIndices').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_rdd = lda_topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "    .map(lambda row: row['termIndices'])\\\n",
    "    .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "    .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(wordVectors)\n",
    "transformed.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_array = udf(lambda v: v.toArray().tolist(), T.ArrayType(T.FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = udf(lambda x: x.index(__builtin__.max(x))\n",
    "                if x is not None else None, T.IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_index = udf(lambda x: 'topic'+str(x), T.StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_word = udf(lambda x: ', '.join(topics_words[x]), T.StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed=transformed.withColumn('topicDistribution_array', to_array(transformed['topicDistribution']))\n",
    "transformed=transformed.withColumn('dominant_topic_index', max_index(transformed['topicDistribution_array']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed=transformed.withColumn('dominant_topic', topic_index(transformed['dominant_topic_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed=transformed.withColumn('dominant_topic_keywords', key_word(transformed['dominant_topic_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.select('topicDistribution_array','dominant_topic','dominant_topic_keywords').show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.groupBy(\"created_date\",\"dominant_topic\",\"dominant_topic_keywords\").count().orderBy(\"created_date\",\"dominant_topic\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.groupBy(\"created_date\",\"dominant_topic\",\"dominant_topic_keywords\").count().toPandas().to_csv('NovemberTopics.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import config\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "detector = Translator()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Getting Topic from User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = input(\"Enter topic to analyze sentiment over: \")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Fetching Tweets and Analyzing Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noOfTweets = 38640  # number of tweets to iterate over\n",
    "\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "\n",
    "for tweet in tweet_text.collect():\n",
    "    dec_lan = detector.detect(tweet[\"cleaned_Text\"])\n",
    "\n",
    "    if dec_lan.lang == \"en\" and dec_lan.confidence == 1:\n",
    "       \n",
    "    \n",
    "        print(tweet[\"cleaned_Text\"])\n",
    "    #     tweet_list.append(tweet.text)\n",
    "    #     analysis = TextBlob(tweet.text)\n",
    "    #     score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "    #     neg = score['neg']\n",
    "    #     neu = score['neu']\n",
    "    #     pos = score['pos']\n",
    "    #     comp = score['compound']\n",
    "    #     polarity += analysis.sentiment.polarity\n",
    "    \n",
    "    #     if (neg > pos):\n",
    "    #         negative_list.append(tweet.text)\n",
    "    #         negative += 1\n",
    "    #     elif (neg < pos):\n",
    "    #         positive_list.append(tweet.text)\n",
    "    #         positive += 1\n",
    "    #     else:\n",
    "    #         neutral_list.append(tweet.text)\n",
    "    #         neutral += 1\n",
    "    else: continue\n",
    "\n",
    "# positive = percentage(positive, noOfTweets)\n",
    "# negative = percentage(negative, noOfTweets)\n",
    "# neutral = percentage(neutral, noOfTweets)\n",
    "# polarity = percentage(polarity, noOfTweets)\n",
    "# positive = format(positive, '.1f')\n",
    "# negative = format(negative, '.1f')\n",
    "# neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
    "\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "\n",
    "print(\"total number: \",len(tweet_list))\n",
    "print(\"positive number: \",len(positive_list))\n",
    "print(\"negative number: \", len(negative_list))\n",
    "print(\"neutral number: \",len(neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating PieCart\n",
    "\n",
    "labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['yellowgreen', 'blue','red']\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for Topic = \"+topic+\"\" )\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
